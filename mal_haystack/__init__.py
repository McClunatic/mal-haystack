"""The MAL Haystack package."""

import pathlib
import zipfile

from typing import Any, Dict, List, Optional

import langdetect
import pandas as pd

from haystack import Document
from haystack.nodes import BaseComponent
from haystack.nodes.file_converter.base import KNOWN_LIGATURES
from tqdm.auto import tqdm


class ZipLister(BaseComponent):
    """Component for generating a list of file names in zip files.

    .. note::

       This expander does not consider depth and reports full paths in the zip.

    Parameters:
        id_hash_keys: Generate the document id from a custom list of strings
            that refer to the document's attributes. If you want to ensure you
            don't have duplicate documents in your DocumentStore but texts are
            not unique, you can modify the metadata and pass e.g. `"meta"` to
            this field (e.g. [`"content"`, `"meta"`]). In this case the id will
            be generated by using the content and the defined metadata.
    """

    outgoing_edges: int = 1

    def __init__(
        self,
        id_hash_keys: Optional[List[str]] = None,
    ):
        """Constructor."""

        self.id_hash_keys = id_hash_keys

    def convert(
        self,
        zip_path: str | pathlib.Path,
        valid_names: Optional[List[str]] = None,
        id_hash_keys: Optional[List[str]] = None,
    ):
        """Convert `zip_path` to a list of contained files.

        Arguments:
            zip_path: The source zip file to produce a namelist for.
            valid_names: If specified, a namelist filter to apply.
            id_hash_keys: Generate the document id from a custom list of
                strings that refer to the document's attributes. If you want to
                ensure you don't have duplicate documents in your DocumentStore
                but texts are not unique, you can modify the metadata and pass
                e.g. `"meta"` to this field (e.g. [`"content"`, `"meta"`]). In
                this case the id will be generated by using the content and the
                defined metadata.
        """

        if id_hash_keys is None:
            id_hash_keys = self.id_hash_keys

        with zipfile.ZipFile(zip_path) as myzip:
            namelist = myzip.namelist()

        if valid_names is not None:
            namelist = [name for name in namelist if name in valid_names]

        return namelist

    def run(
        self,
        file_paths: List[str | pathlib.Path],
        valid_names: Optional[List[str]] = None,
        id_hash_keys: Optional[List[str]] = None,
    ):
        """Extract contained filenames from `zip_path`.

        Arguments:
            zip_path: The source zip file to produce a namelist for.
            valid_names: If specified, a namelist filter to apply.
            id_hash_keys: Generate the document id from a custom list of
                strings that refer to the document's attributes. If you want to
                ensure you don't have duplicate documents in your DocumentStore
                but texts are not unique, you can modify the metadata and pass
                e.g. `"meta"` to this field (e.g. [`"content"`, `"meta"`]). In
                this case the id will be generated by using the content and the
                defined metadata.
        """

        # Convert series into documents
        namelist = []
        zip_path_list = []
        for zip_path in file_paths:
            nl = self.convert(
                zip_path,
                valid_names=valid_names,
                id_hash_keys=id_hash_keys,
            )

            zp_list = [zip_path] * len(nl)
            namelist.extend(nl)
            zip_path_list.extend(zp_list)

        result = {'file_paths': namelist, 'meta': {'zip_paths': zip_path_list}}
        return result, 'output_1'

    run_batch = run


class ZipDataFramer(BaseComponent):
    """Component for generating a set of dataframes from zipped CSV files.

    Parameters:
        id_hash_keys: Generate the document id from a custom list of strings
            that refer to the document's attributes. If you want to ensure you
            don't have duplicate documents in your DocumentStore but texts are
            not unique, you can modify the metadata and pass e.g. `"meta"` to
            this field (e.g. [`"content"`, `"meta"`]). In this case the id will
            be generated by using the content and the defined metadata.
    """

    outgoing_edges: int = 1

    def __init__(
        self,
        id_hash_keys: Optional[List[str]] = None,
    ):
        """Constructor."""

        self.id_hash_keys = id_hash_keys

    def convert(
        self,
        file_paths: str | pathlib.Path,
        meta: Dict[str, List[str | pathlib.Path]],
        id_hash_keys: Optional[List[str]] = None,
    ):
        """Extract `file_paths` data as :class:`pandas.DataFrame` objects.

        .. note::

           The ``meta` dictionary must contain the `zip_path` key whose value
           indicates the location of the zip file containing `file_paths`!

        Arguments:
            file_paths: Path of the zipped CSV file.
            meta: Dictionary containing ``zip_paths`` key to zip file paths to
                extract CSV `file_paths` from.
            id_hash_keys: Generate the document id from a custom list of
                strings that refer to the document's attributes. If you want to
                ensure you don't have duplicate documents in your DocumentStore
                but texts are not unique, you can modify the metadata and pass
                e.g. `"meta"` to this field (e.g. [`"content"`, `"meta"`]). In
                this case the id will be generated by using the content and the
                defined metadata.
        """

        if id_hash_keys is None:
            id_hash_keys = self.id_hash_keys

        if not meta or 'zip_paths' not in meta:
            raise ValueError('`meta` dict must contain "zip_paths" key!')

        dfs = []
        for file_path, zip_path in zip(file_paths, meta['zip_paths']):
            with zipfile.ZipFile(zip_path) as myzip:
                with myzip.open(file_path) as myzipfile:
                    dfs.append(pd.read_csv(myzipfile))

        return dfs

    def run(
        self,
        file_paths: str | pathlib.Path,
        meta: Dict[str, str | pathlib.Path],
        id_hash_keys: Optional[List[str]] = None,
    ):
        """Extract contained filenames from `zip_path`.

        Arguments:
            file_paths: Path of the zipped CSV file.
            meta: Dictionary containing ``zip_path`` key to zip file path to
                extract CSV `file_paths` from.
            id_hash_keys: Generate the document id from a custom list of
                strings that refer to the document's attributes. If you want to
                ensure you don't have duplicate documents in your DocumentStore
                but texts are not unique, you can modify the metadata and pass
                e.g. `"meta"` to this field (e.g. [`"content"`, `"meta"`]). In
                this case the id will be generated by using the content and the
                defined metadata.
        """

        # Convert paths into dataframes
        dfs = self.convert(
            file_paths,
            meta,
            id_hash_keys=id_hash_keys,
        )

        result = {'dataframes': dfs}
        return result, 'output_1'

    run_batch = run


class SeriesConverter(BaseComponent):
    """Component for converting :class:`pandas.DataFrame` and
    :class:`pandas.Series` to :class:`haystack.Document` objects.

    Parameters:
        valid_languages: Valid languages from list specified in ISO 639-1
            (https://en.wikipedia.org/wiki/ISO_639-1) format.
            This option can be used to add test for encoding errors. If the
            extracted text is not one of the valid languages, then it might
            likely be encoding error resulting in garbled text.
        id_hash_keys: Generate the document id from a custom list of strings
            that refer to the document's attributes. If you want to ensure you
            don't have duplicate documents in your DocumentStore but texts are
            not unique, you can modify the metadata and pass e.g. `"meta"` to
            this field (e.g. [`"content"`, `"meta"`]). In this case the id will
            be generated by using the content and the defined metadata.
        progress_bar: Show a progress bar for the conversion.
    """

    outgoing_edges: int = 1

    def __init__(
        self,
        valid_languages: Optional[List[str]] = None,
        id_hash_keys: Optional[List[str]] = None,
        progress_bar: bool = True,
    ):
        """Constructor."""

        self.valid_languages = valid_languages
        self.id_hash_keys = id_hash_keys
        self.progress_bar = progress_bar

    def validate_language(
        self,
        text: str,
        valid_languages: Optional[List[str]] = None,
    ) -> bool:
        """Validate if the language of the text is one of valid languages.

        Arguments:
            text: The text to validate.
            valid_languages: Valid languages from list specified in ISO 639-1
                (https://en.wikipedia.org/wiki/ISO_639-1) format.
                This option can be used to add test for encoding errors. If the
                extracted text is not one of the valid languages, then it might
                likely be encoding error resulting in garbled text.
        """
        if valid_languages is None:
            valid_languages = self.valid_languages

        if not valid_languages:
            return True

        try:
            lang = langdetect.detect(text)
        except langdetect.lang_detect_exception.LangDetectException:
            lang = None

        return lang in valid_languages

    def convert(
        self,
        series: pd.Series | str,
        meta: Optional[Dict[str, Any]],
        dataframe: Optional[pd.DataFrame] = None,
        dataframe_meta_cols: Optional[List[str]] = None,
        valid_languages: Optional[List[str]] = None,
        id_hash_keys: Optional[List[str]] = None,
    ):
        """Convert `series` to a list of :class:`haystack.Document` objects.

        Arguments:
            series: The source series or `dataframe` column to convert.
            meta: Dictionary of meta key-value pairs to append in the returned
                document.
            dataframe: The source :class:`pandas.DataFrame` to extract `series`
                column from. This argument is required when `series` is a
                string.
            dataframe_meta_cols: If `dataframe` is a :class:`pandas.DataFrame`,
                the additional metadata columns to append in the returned
                documents.
            valid_languages: Valid languages from list specified in ISO 639-1
                (https://en.wikipedia.org/wiki/ISO_639-1) format.
                This option can be used to add test for encoding errors. If the
                extracted text is not one of the valid languages, then it might
                likely be encoding error resulting in garbled text.
            id_hash_keys: Generate the document id from a custom list of
                strings that refer to the document's attributes. If you want to
                ensure you don't have duplicate documents in your DocumentStore
                but texts are not unique, you can modify the metadata and pass
                e.g. `"meta"` to this field (e.g. [`"content"`, `"meta"`]). In
                this case the id will be generated by using the content and the
                defined metadata.
        """

        if valid_languages is None:
            valid_languages = self.valid_languages
        if id_hash_keys is None:
            id_hash_keys = self.id_hash_keys

        if isinstance(series, str):
            if dataframe is None:
                raise ValueError(
                    'Argument `dataframe` is required when `series` is of '
                    'type `str`!'
                )
            series = dataframe[series]

        documents = []
        for idx, content in tqdm(
            series.items(),
            total=len(series),
            disable=not self.progress_bar,
            desc='Extracting Series text',
        ):
            metadata = None
            if dataframe is not None and dataframe_meta_cols:
                cols = dataframe_meta_cols
                metadata = dataframe.loc[idx][cols].to_dict()
            documents.append(Document(
                content=content,
                content_type='text',
                meta=metadata,
                id_hash_keys=id_hash_keys,
            ))

        return documents

    def run(
        self,
        series: List[pd.Series] | List[str] | str,
        meta: Optional[List[str]],
        dataframes: Optional[List[pd.DataFrame]] = None,
        valid_languages: Optional[List[str]] = None,
        id_hash_keys: Optional[List[str]] = None,
        known_ligatures: Dict[str, str] = KNOWN_LIGATURES,
    ):
        """Extract text from a :class:`pandas.Series` object.

        Arguments:
            series: The source series objects or `dataframe` column names to
                extract text from.
            meta: If `dataframe` is a :class:`pandas.DataFrame`, the additional
                metadata columns to append in the returned documents.
            dataframes: The source :class:`pandas.DataFrame` objects to extract
                `series` columns from. This argument is required when `series`
                is a string or list of strings.
            valid_languages: Valid languages from list specified in ISO 639-1
                (https://en.wikipedia.org/wiki/ISO_639-1) format.
                This option can be used to add test for encoding errors. If the
                extracted text is not one of the valid languages, then it might
                likely be encoding error resulting in garbled text.
            id_hash_keys: Generate the document id from a custom list of
                strings that refer to the document's attributes. If you want to
                ensure you don't have duplicate documents in your DocumentStore
                but texts are not unique, you can modify the metadata and pass
                e.g. `"meta"` to this field (e.g. [`"content"`, `"meta"`]). In
                this case the id will be generated by using the content and the
                defined metadata.
            known_ligatures: Some converters tends to recognize clusters of
                letters as ligatures, such as "ﬀ" (double f). Such ligatures
                however make text hard to compare with the content of other
                files, which are generally ligature free. Therefore we
                automatically find and replace the most common ligatures with
                their split counterparts. The default mapping is in
                `haystack.nodes.file_converter.base.KNOWN_LIGATURES`: it is
                rather biased towards Latin alphabeths but excludes all
                ligatures that are known to be used in IPA. You can use this
                parameter to provide your own set of ligatures to clean up from
                the documents.
        """

        # Convert series into documents
        if isinstance(series, str):
            series = [series] * len(dataframes)

        documents = []
        for serie, dataframe in zip(series, dataframes):
            documents.extend(self.convert(
                serie,
                meta=meta,
                dataframe=dataframe,
                valid_languages=valid_languages,
                id_hash_keys=id_hash_keys,
            ))

        # Cleanup ligatures
        for document in documents:
            for lig, letters in known_ligatures.items():
                if document.content is not None:
                    document.content = document.content.replace(lig, letters)

        result = {'documents': documents}
        return result, 'output_1'

    run_batch = run
