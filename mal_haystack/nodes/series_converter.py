from typing import Any, Dict, List, Optional

import langdetect
import pandas as pd
from haystack import Document
from haystack.nodes import BaseComponent
from haystack.nodes.file_converter.base import KNOWN_LIGATURES
from tqdm.auto import tqdm


class SeriesConverter(BaseComponent):
    """Component for converting :class:`pandas.DataFrame` and
    :class:`pandas.Series` to :class:`haystack.Document` objects.

    Parameters:
        valid_languages: Valid languages from list specified in ISO 639-1
            (https://en.wikipedia.org/wiki/ISO_639-1) format.
            This option can be used to add test for encoding errors. If the
            extracted text is not one of the valid languages, then it might
            likely be encoding error resulting in garbled text.
        id_hash_keys: Generate the document id from a custom list of strings
            that refer to the document's attributes. If you want to ensure you
            don't have duplicate documents in your DocumentStore but texts are
            not unique, you can modify the metadata and pass e.g. `"meta"` to
            this field (e.g. [`"content"`, `"meta"`]). In this case the id will
            be generated by using the content and the defined metadata.
        progress_bar: Show a progress bar for the conversion.
    """

    outgoing_edges: int = 1

    def __init__(
        self,
        valid_languages: Optional[List[str]] = None,
        id_hash_keys: Optional[List[str]] = None,
        progress_bar: bool = True,
    ):
        """Constructor."""

        self.valid_languages = valid_languages
        self.id_hash_keys = id_hash_keys
        self.progress_bar = progress_bar

    def validate_language(
        self,
        text: str,
        valid_languages: Optional[List[str]] = None,
    ) -> bool:
        """Validate if the language of the text is one of valid languages.

        Arguments:
            text: The text to validate.
            valid_languages: Valid languages from list specified in ISO 639-1
                (https://en.wikipedia.org/wiki/ISO_639-1) format.
                This option can be used to add test for encoding errors. If the
                extracted text is not one of the valid languages, then it might
                likely be encoding error resulting in garbled text.
        """
        if valid_languages is None:
            valid_languages = self.valid_languages

        if not valid_languages:
            return True

        try:
            lang = langdetect.detect(text)
        except langdetect.lang_detect_exception.LangDetectException:
            lang = None

        return lang in valid_languages

    def convert(
        self,
        series: pd.Series | str,
        meta: Optional[Dict[str, Any]],
        dataframe: Optional[pd.DataFrame] = None,
        dataframe_meta_cols: Optional[List[str]] = None,
        valid_languages: Optional[List[str]] = None,
        id_hash_keys: Optional[List[str]] = None,
    ):
        """Convert `series` to a list of :class:`haystack.Document` objects.

        Arguments:
            series: The source series or `dataframe` column to convert.
            meta: Dictionary of meta key-value pairs to append in the returned
                document.
            dataframe: The source :class:`pandas.DataFrame` to extract `series`
                column from. This argument is required when `series` is a
                string.
            dataframe_meta_cols: If `dataframe` is a :class:`pandas.DataFrame`,
                the additional metadata columns to append in the returned
                documents.
            valid_languages: Valid languages from list specified in ISO 639-1
                (https://en.wikipedia.org/wiki/ISO_639-1) format.
                This option can be used to add test for encoding errors. If the
                extracted text is not one of the valid languages, then it might
                likely be encoding error resulting in garbled text.
            id_hash_keys: Generate the document id from a custom list of
                strings that refer to the document's attributes. If you want to
                ensure you don't have duplicate documents in your DocumentStore
                but texts are not unique, you can modify the metadata and pass
                e.g. `"meta"` to this field (e.g. [`"content"`, `"meta"`]). In
                this case the id will be generated by using the content and the
                defined metadata.
        """

        if valid_languages is None:
            valid_languages = self.valid_languages
        if id_hash_keys is None:
            id_hash_keys = self.id_hash_keys

        if isinstance(series, str):
            if dataframe is None:
                raise ValueError(
                    'Argument `dataframe` is required when `series` is of '
                    'type `str`!'
                )
            series = dataframe[series]

        documents = []
        for idx, content in tqdm(
            series.items(),
            total=len(series),
            disable=not self.progress_bar,
            desc='Extracting Series text',
        ):
            metadata = None
            if dataframe is not None and dataframe_meta_cols:
                cols = dataframe_meta_cols
                metadata = dataframe.loc[idx][cols].to_dict()
            documents.append(Document(
                content=content,
                content_type='text',
                meta=metadata,
                id_hash_keys=id_hash_keys,
            ))

        return documents

    def run(
        self,
        series: List[pd.Series] | List[str] | str,
        meta: Optional[List[str]],
        dataframes: Optional[List[pd.DataFrame]] = None,
        valid_languages: Optional[List[str]] = None,
        id_hash_keys: Optional[List[str]] = None,
        known_ligatures: Dict[str, str] = KNOWN_LIGATURES,
    ):
        """Extract text from a :class:`pandas.Series` object.

        Arguments:
            series: The source series objects or `dataframe` column names to
                extract text from.
            meta: If `dataframe` is a :class:`pandas.DataFrame`, the additional
                metadata columns to append in the returned documents.
            dataframes: The source :class:`pandas.DataFrame` objects to extract
                `series` columns from. This argument is required when `series`
                is a string or list of strings.
            valid_languages: Valid languages from list specified in ISO 639-1
                (https://en.wikipedia.org/wiki/ISO_639-1) format.
                This option can be used to add test for encoding errors. If the
                extracted text is not one of the valid languages, then it might
                likely be encoding error resulting in garbled text.
            id_hash_keys: Generate the document id from a custom list of
                strings that refer to the document's attributes. If you want to
                ensure you don't have duplicate documents in your DocumentStore
                but texts are not unique, you can modify the metadata and pass
                e.g. `"meta"` to this field (e.g. [`"content"`, `"meta"`]). In
                this case the id will be generated by using the content and the
                defined metadata.
            known_ligatures: Some converters tends to recognize clusters of
                letters as ligatures, such as "ï¬€" (double f). Such ligatures
                however make text hard to compare with the content of other
                files, which are generally ligature free. Therefore we
                automatically find and replace the most common ligatures with
                their split counterparts. The default mapping is in
                `haystack.nodes.file_converter.base.KNOWN_LIGATURES`: it is
                rather biased towards Latin alphabeths but excludes all
                ligatures that are known to be used in IPA. You can use this
                parameter to provide your own set of ligatures to clean up from
                the documents.
        """

        # Convert series into documents
        if isinstance(series, str):
            series = [series] * len(dataframes)

        documents = []
        for serie, dataframe in zip(series, dataframes):
            documents.extend(self.convert(
                serie,
                meta=meta,
                dataframe=dataframe,
                valid_languages=valid_languages,
                id_hash_keys=id_hash_keys,
            ))

        # Cleanup ligatures
        for document in documents:
            for lig, letters in known_ligatures.items():
                if document.content is not None:
                    document.content = document.content.replace(lig, letters)

        result = {'documents': documents}
        return result, 'output_1'

    run_batch = run
