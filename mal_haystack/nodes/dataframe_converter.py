"""The MAL Haystack DataFrame to document converter node."""

from typing import Dict, List, Optional

import langdetect
import pandas as pd
from haystack import Document
from haystack.nodes import BaseComponent
from haystack.nodes.file_converter.base import KNOWN_LIGATURES
# TODO: Investigate why tqdm.auto does not work properly with VS Code
from tqdm import tqdm


class DataFrameConverter(BaseComponent):
    """Component for converting :class:`pandas.DataFrame` objects
    to :class:`haystack.Document` objects.

    Parameters:
        document_column: The DataFrame column containing text to convert
            to :class:`haystack.Document` objects.
        meta_columns: Additional DataFrame metadata columns to append
            in the returned documents.
        valid_languages: Valid languages from list specified in ISO 639-1
            (https://en.wikipedia.org/wiki/ISO_639-1) format.
            This option can be used to add test for encoding errors. If the
            extracted text is not one of the valid languages, then it might
            likely be encoding error resulting in garbled text.
        id_hash_keys: Generate the document id from a custom list of strings
            that refer to the document's attributes. If you want to ensure you
            don't have duplicate documents in your DocumentStore but texts are
            not unique, you can modify the metadata and pass e.g. `"meta"` to
            this field (e.g. [`"content"`, `"meta"`]). In this case the id will
            be generated by using the content and the defined metadata.
        progress_bar: Show a progress bar for the conversion.
    """

    outgoing_edges: int = 1

    def __init__(
        self,
        document_column: Optional[str] = None,
        meta_columns: Optional[List[str]] = None,
        valid_languages: Optional[List[str]] = None,
        id_hash_keys: Optional[List[str]] = None,
        progress_bar: bool = True,
    ):
        """Constructor."""

        self.document_column = document_column
        self.meta_columns = meta_columns
        self.valid_languages = valid_languages
        self.id_hash_keys = id_hash_keys
        self.progress_bar = progress_bar

    def validate_language(
        self,
        text: str,
        valid_languages: Optional[List[str]] = None,
    ) -> bool:
        """Validate if the language of the text is one of valid languages.

        Arguments:
            text: The text to validate.
            valid_languages: Valid languages from list specified in ISO 639-1
                (https://en.wikipedia.org/wiki/ISO_639-1) format.
                This option can be used to add test for encoding errors. If the
                extracted text is not one of the valid languages, then it might
                likely be encoding error resulting in garbled text.
        """
        if valid_languages is None:
            valid_languages = self.valid_languages

        if not valid_languages:
            return True

        try:
            lang = langdetect.detect(text)
        except langdetect.lang_detect_exception.LangDetectException:
            lang = None

        return lang in valid_languages

    def convert(
        self,
        dataframe: Optional[pd.DataFrame] = None,
        document_column: Optional[str] = None,
        meta_columns: Optional[List[str]] = None,
        valid_languages: Optional[List[str]] = None,
        id_hash_keys: Optional[List[str]] = None,
    ):
        """Convert `series` to a list of :class:`haystack.Document` objects.

        Arguments:
            dataframe: The source :class:`pandas.DataFrame` to extract `series`
                column from. This argument is required when `series` is a
                string.
            document_column: The `dataframe` column containing text to convert
                to :class:`haystack.Document` objects.
            meta_columns: Additional `dataframe` metadata columns to append
                in the returned documents.
            valid_languages: Valid languages from list specified in ISO 639-1
                (https://en.wikipedia.org/wiki/ISO_639-1) format.
                This option can be used to add test for encoding errors. If the
                extracted text is not one of the valid languages, then it might
                likely be encoding error resulting in garbled text.
            id_hash_keys: Generate the document id from a custom list of
                strings that refer to the document's attributes. If you want to
                ensure you don't have duplicate documents in your DocumentStore
                but texts are not unique, you can modify the metadata and pass
                e.g. `"meta"` to this field (e.g. [`"content"`, `"meta"`]). In
                this case the id will be generated by using the content and the
                defined metadata.
        """

        if document_column is None:
            document_column = self.document_column
        if meta_columns is None:
            meta_columns = self.meta_columns
        if valid_languages is None:
            valid_languages = self.valid_languages
        if id_hash_keys is None:
            id_hash_keys = self.id_hash_keys

        series = dataframe[document_column]

        documents = []
        for idx, content in tqdm(
            series.items(),
            total=len(series),
            disable=not self.progress_bar,
            desc='Extracting DataFrame documents',
        ):
            metadata = {'index': idx}
            if dataframe is not None and meta_columns:
                cols = meta_columns
                metadata |= dataframe.loc[idx][cols].to_dict()
            documents.append(Document(
                content=content,
                content_type='text',
                meta=metadata,
                id_hash_keys=id_hash_keys,
            ))

        return documents

    def run(
        self,
        dataframes: Optional[List[pd.DataFrame]] = None,
        document_column: Optional[str] = None,
        meta_columns: Optional[List[str]] = None,
        valid_languages: Optional[List[str]] = None,
        id_hash_keys: Optional[List[str]] = None,
        known_ligatures: Dict[str, str] = KNOWN_LIGATURES,
    ):
        """Extract text from a :class:`pandas.Series` object.

        Arguments:
            dataframes: The source :class:`pandas.DataFrame` objects to extract
                `series` columns from. This argument is required when `series`
                is a string or list of strings.
            document_column: The `dataframe` column containing text to convert
                to :class:`haystack.Document` objects.
            meta_columns: Additional `dataframe` metadata columns to append
                in the returned documents.
            valid_languages: Valid languages from list specified in ISO 639-1
                (https://en.wikipedia.org/wiki/ISO_639-1) format.
                This option can be used to add test for encoding errors. If the
                extracted text is not one of the valid languages, then it might
                likely be encoding error resulting in garbled text.
            id_hash_keys: Generate the document id from a custom list of
                strings that refer to the document's attributes. If you want to
                ensure you don't have duplicate documents in your DocumentStore
                but texts are not unique, you can modify the metadata and pass
                e.g. `"meta"` to this field (e.g. [`"content"`, `"meta"`]). In
                this case the id will be generated by using the content and the
                defined metadata.
            known_ligatures: Some converters tends to recognize clusters of
                letters as ligatures, such as "ï¬€" (double f). Such ligatures
                however make text hard to compare with the content of other
                files, which are generally ligature free. Therefore we
                automatically find and replace the most common ligatures with
                their split counterparts. The default mapping is in
                `haystack.nodes.file_converter.base.KNOWN_LIGATURES`: it is
                rather biased towards Latin alphabeths but excludes all
                ligatures that are known to be used in IPA. You can use this
                parameter to provide your own set of ligatures to clean up from
                the documents.
        """

        documents = []
        for dataframe in dataframes:
            documents.extend(self.convert(
                dataframe,
                document_column=document_column,
                meta_columns=meta_columns,
                valid_languages=valid_languages,
                id_hash_keys=id_hash_keys,
            ))

        # Cleanup ligatures
        for document in documents:
            for lig, letters in known_ligatures.items():
                if document.content is not None:
                    document.content = document.content.replace(lig, letters)

        result = {'documents': documents}
        return result, 'output_1'

    run_batch = run
